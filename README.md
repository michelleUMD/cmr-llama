# CMR NER with LLaMA
Fine-tune LLaMA models to extract structured, JSON-style data from medical reports. Inputs are XML files generated by MedTator, and outputs are parsed strings in JSON format ready for downstream analysis.

## Table of Contents

1. [Features](#features)
2. [Project Structure](#project-structure)
3. [Environment Setup](#environment-setup)
4. [Usage](#usage)

   * [Training](#training)
   * [Inference](#inference)
   * [Post-processing](#post-processing)

---

## Features

* Fine-tune LLaMA models on annotated medical text.
* Accepts MedTator XML inputs and produces structured, JSON-style outputs.
* Model weights located at https://huggingface.co/michelleUMD/cmr-llama/

---

## Project Structure

```plaintext
.
├── utils.py             # Shared functions and classes
├── train.py             # Fine-tuning script
├── inference.py         # Inference script
├── post_create_csv.py   # Converts model output strings to CSV tables
└── README.md            # This documentation
```

---

Install Python dependencies with:

```bash
pip install -r requirements.txt
```

---

## Environment Setup

Activate your virtual environment before running any scripts. For example:

```bash
# Bash example
source /file_path/.../myenv/bin/activate
```

---

## Usage

### Training

```bash
python train.py \
  --model_name "meta-llama/Llama-3.3-70B-Instruct" \
  --model_type "llama3.3" \
  --use_4bit True \
  --gradient_checkpointing True \
  --output_dir "./results_llama33" \
  --bf16 False \
  --fp16 True
```

| Flag                       | Description                                     |
| -------------------------- | ----------------------------------------------- |
| `--model_name`             | Hugging Face identifier of the base LLaMA model |
| `--model_type`             | Custom alias for model architecture             |
| `--use_4bit`               | Enable 4-bit quantization                       |
| `--gradient_checkpointing` | Save GPU memory by checkpointing activations    |
| `--output_dir`             | Directory to save checkpoints and logs          |
| `--bf16`, `--fp16`         | Mixed-precision settings                        |

---

### Inference

```bash
python inference.py \
  --model_name "meta-llama/Llama-3.3-70B-Instruct" \
  --model_type "llama3.3" \
  --use_4bit True \
  --bf16 False \
  --fp16 True \
  --per_device_eval_batch_size 8
```

Adjust `--per_device_eval_batch_size` based on your GPU memory.

---

### Post-processing

After inference, convert the JSON-style output strings into tabular format:

```bash
python post_create_csv.py 
```

---
